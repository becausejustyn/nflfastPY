{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first in a series of posts that I plan on writing as I learn how to apply Bayesian methods to different topics/problems that I find interesting. In this post I go over how to use the Bayesian bootstrap to get measure of uncertainty for an NFL quarterback's (QB) yards per pass attempt (YPA).\n",
    "\n",
    "\n",
    "## What is the Bayesian Bootstrap and how do we compute it?\n",
    "\n",
    "Bootstrapping is a resampling technique that allows us to calculate the uncertainty for a given statistic of interest (e.g. mean, median, etc.).  In the classical bootstrap we construct these measures of uncertainty by first creating multiple datasets, called bootstrap samples, by sampling with replacement from the original data. Then for each of these newly generated samples, we calculate the statistic of interest and end up with an approximation of its distribution.\n",
    "\n",
    "Here is an example of the classical bootstrap being used to construct an interval around a regression line:\n",
    "\n",
    "In the classical bootstrap, sampling with replacement can be seen as [applying weights to our observations, based on the (normalized) counts we draw from a multinomial distribution](http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/#its-the-bayesian-bootstrap-but-with-discrete-weights).  In the Bayesian bootstrap, instead of applying weights from a [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) we apply weights from a uniform (flat) [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution). ([Here's](https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution-as-a-probability-distribution-over-the-k-%E2%88%92-1-dimensional-probability-simplex) an intuitive explanation of the Dirichlet distribution for those who don't know what it is). To create Bayesian bootstrap samples, we repeat the following procedure as many times as we'd like (the more times we repeat it the better):\n",
    "\n",
    "1. Draw weights from a uniform Dirichlet distribution with the same dimension as the number of observations in the data.\n",
    "    - **NOTE:** Here the uniform Dirichlet distribution acts as an uninformative prior.\n",
    "2. If possible, calculate the statistic using the weights from the Dirichlet distribution.\n",
    "3. Otherwise, if the statistic doesn’t directly use weights in its calculation do the following:\n",
    "    1. Resample the data according to the weights drawn from the Dirichlet distribution.\n",
    "        - **NOTE:** In this step we create as large of a sample as possible. It should as large as the original dataset.\n",
    "    2. Use the resampled data to calculate the statistic.\n",
    "\n",
    "The final distribution created by the Bayesian bootstrap is a [posterior distribution](https://en.wikipedia.org/wiki/Posterior_probability) of the statistic of interest.\n",
    "\n",
    "To get a better sense of the above procedure let’s code up our own Bayesian bootstrap function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import bayesian_bootstrap as bb\n",
    "from scipy import stats\n",
    "from astropy.utils import NumpyRNGContext\n",
    "\n",
    "# set up the style for our plots\n",
    "sns.set(style='white', palette='colorblind', font_scale=1.3,\n",
    "        rc={'figure.figsize':(12,9), \n",
    "            \"axes.facecolor\": (0, 0, 0, 0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_boot(X, statistic, n_samples1=1000, n_samples2=1000, weight_kwd=None,\n",
    "               *args, **kwargs):\n",
    "        \n",
    "    # draw our weights from the uniform dirichlet distribution\n",
    "    # [1]*len(X) is the dimension of the distribution\n",
    "    # n_samples1 represents the number of bootstrap replications to perform\n",
    "    # from the bayesian perspective think of it as the number of draws from the\n",
    "    # posterior distribution\n",
    "    # in terms of the classical bootstrap this is the number times the data is \n",
    "    # resampled\n",
    "    weights = np.random.dirichlet([1]*len(X), n_samples1)\n",
    "    \n",
    "    # if the statistic function accepts weights, use them to calculate the \n",
    "    # bayesian bootstrap samples\n",
    "    if weight_kwd is not None:\n",
    "        samples = [statistic(X, *args, **{weight_kwd: w}, **kwargs) for w in weights]\n",
    "    \n",
    "    # otherwise we have to do a weighted resampling of the data, based on\n",
    "    # the weights we drew from the dirichlet distribution\n",
    "    else:\n",
    "        samples = []\n",
    "        for w in weights:\n",
    "            # resample the indexes using the dirchlet weights\n",
    "            # the greater n_sample2 is, the better\n",
    "            sample_idx = np.random.choice(range(len(X)), p=w, size=n_samples2)\n",
    "            X_resample = X[sample_idx]\n",
    "            # calculate the statistic on the resampled data and append it\n",
    "            # to our samples list\n",
    "            samples.append(statistic(X, *args, **kwargs))\n",
    "\n",
    "    return np.array(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's set up our data. The play by play data comes from [Ron Yurko's](https://twitter.com/Stat_Ron) awesome [nflscrapR-data github repository](https://github.com/ryurko/nflscrapR-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and set up our data. \n",
    "pbp_df = pd.read_csv('data/pbp_2017.csv', low_memory=False)\n",
    "rosters_df = pd.read_csv('data/team_2017_rosters.csv')\n",
    "\n",
    "# replace . with _\n",
    "pbp_df.columns = pbp_df.columns.str.replace('.', '_')\n",
    "\n",
    "# keep all qb pass attempt\n",
    "# first we keep the plays where a pass occured\n",
    "# then we get the passer's position (along with their full name and GSIS_ID)\n",
    "# in order to filter out all non-QB pass attempts\n",
    "qb_pass_df = (pbp_df.query('PassAttempt == 1')\n",
    "                    .merge(rosters_df[['GSIS_ID', 'Player', 'Pos']], how='left', \n",
    "                           left_on='Passer_ID', right_on='GSIS_ID')\n",
    "                    .query('Pos == \"QB\"'))\n",
    "\n",
    "# some plays are labeled as both a sack and a pass attempts, they should be\n",
    "# one or the other\n",
    "# For the 2017 pbp data I found 17 instances where this mislabeling occurs\n",
    "# I manually checked the description in another notebook, \n",
    "# they tend to be plays that were challenged and reversed\n",
    "# here I correct the issue\n",
    "sack_and_pass_mask = (qb_pass_df.Sack==1) & (qb_pass_df.PassAttempt==1)\n",
    "corrected_sack = np.array([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1])\n",
    "corrected_pass = 1 - corrected_sack\n",
    "qb_pass_df.loc[sack_and_pass_mask, 'Sack'] = corrected_sack\n",
    "qb_pass_df.loc[sack_and_pass_mask, 'PassAttempt'] = corrected_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brees_df = qb_pass_df.loc[qb_pass_df.Player == 'Drew Brees']\n",
    "watson_df = qb_pass_df.loc[qb_pass_df.Player == 'Deshaun Watson']\n",
    "\n",
    "brees_ypa = brees_df.Yards_Gained.sum() / brees_df.Yards_Gained.size\n",
    "watson_ypa = watson_df.Yards_Gained.sum() / watson_df.Yards_Gained.size\n",
    "\n",
    "print(f'Brees threw {brees_df.Yards_Gained.sum()} total yards on {brees_df.Yards_Gained.size} pass attempts, averaging {brees_ypa:.2F} yards per attempt')\n",
    "print(f'Watsom threw {watson_df.Yards_Gained.sum()} total yards on {watson_df.Yards_Gained.size} pass attempts, averaging {watson_ypa:.2F} yards per attempt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To construct our Bayesian bootstrap samples for each QB, we pass the yards gained on each of their pass attempts to `bayesian_bootstrap`'s `mean` function and set the number of replications we want.\n",
    "# for reproducibility, set the seed within this context\n",
    "with NumpyRNGContext(42):\n",
    "    brees_bootstrap = bb.mean(brees_df.Yards_Gained, n_replications=10000)\n",
    "    watson_bootstrap = bb.mean(watson_df.Yards_Gained, n_replications=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `mean` returns us a `list` with the mean for each bootstrapped sample.  This `list` is the posterior distribution over the probable mean values of the data we are interested in.\n",
    "# the mean YPA for the first 5 bootstrapped samples\n",
    "brees_bootstrap[:5]\n",
    "\n",
    "# there are a total of 10000 means, 1 for each replication\n",
    "len(brees_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(brees_bootstrap, color='salmon')\n",
    "ax.set(xlabel='YPA', ylabel='Density', title='Drew Brees Mean YPA')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also construct a credible interval. `bayesian_bootstrap` provides two methods to do that, the `central_credible_interval` and the `highest_density_interval` functions. When constructing intervals for unimodal (i.e. it has one peak), symmetric distribution, both methods should give you similar intervals. But if the distribution is multimodal (i.e. it has multiple peaks representing well separated modes), the `central_credible_interval` function will return a single interval while the the `highest_density_interval` will return multiple disjointed intervals centered around the modes of the distribution. A good discussion on these two methods of constructing credible intervals can be found [here](https://stats.stackexchange.com/questions/240749/how-to-find-95-credible-interval) and [here](https://stats.stackexchange.com/questions/24588/quantile-intervals-vs-highest-posterior-density-intervals). Here we will use the `highest_density_interval` function to calculate our credible intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a 95% HDI\n",
    "brees_ci_low, brees_ci_hi = bb.highest_density_interval(brees_bootstrap)\n",
    "print(f'Low CI: {brees_ci_low:.2f}\\n High CI: {brees_ci_hi:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(brees_bootstrap, color='salmon')\n",
    "ax.plot([brees_ci_low, brees_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='YPA', ylabel='Density', title='Drew Brees Mean YPA')\n",
    "sns.despine()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brees' 95% HDI is between 7.33 and 9.04 YPA, which means that according to our model (you can think  of the Bayesian bootstrap as [poor man's model](http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/#some-final-thoughts)) and the data we've observed (Brees' passes), there is a 95% chance that Brees' mean YPA is between 7.33 YPA and 9.04 YPA. \n",
    "\n",
    "Now let's also plot Watson's posterior distribution and compare the distributions for both QBs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a 95% HDI\n",
    "watson_ci_low, watson_ci_hi = bb.highest_density_interval(watson_bootstrap)\n",
    "print('low ci:', watson_ci_low, '\\nhigh ci:', watson_ci_hi)\n",
    "\n",
    "ax = sns.distplot(watson_bootstrap)\n",
    "ax.plot([watson_ci_low, watson_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='YPA', ylabel='Density', title='Deshaun Watson Mean YPA')\n",
    "sns.despine()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(watson_bootstrap, label='Deshaun Watson')\n",
    "ax = sns.distplot(brees_bootstrap, label='Drew Brees', ax=ax, color='salmon')\n",
    "ax.set(xlabel='YPA', ylabel='Density')\n",
    "sns.despine()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watson's posterior distribution is shorter and wider than Brees', which indicates that there is more uncertainty with respect to Watson's mean YPA. \n",
    "\n",
    "# We can also measure the uncertainty in the difference between the player's YPA by subtracting their posteriors.\n",
    "\n",
    "# calculate the posterior for the difference between Watson's and Brees YPA\n",
    "ypa_diff = np.array(watson_bootstrap) - np.array(brees_bootstrap)\n",
    "# get the hdi\n",
    "ypa_diff_ci_low, ypa_diff_ci_hi = bb.highest_density_interval(ypa_diff)\n",
    "\n",
    "# the mean of the posterior\n",
    "print(f'Posterior Mean: {ypa_diff.mean()}')\n",
    "print(f'Low CI: {ypa_diff_ci_low:.2f}\\n High CI: {ypa_diff_ci_hi:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(ypa_diff)\n",
    "ax.plot([ypa_diff_ci_low, ypa_diff_ci_hi], [0, 0], linewidth=10, c='k', marker='o', \n",
    "         label='95% HDI')\n",
    "ax.set(xlabel='YPA', ylabel='Density', \n",
    "       title='The difference between Watson\\'s and Brees\\' mean YPA')\n",
    "sns.despine()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the mean of the posterior is about 0.2 and the 95% HDI is pretty wide, ranging from -1.5 to about 2, indicating to us that we can't really be certain that the two player's mean YPA were that different last season. \n",
    "\n",
    "# We can actually calculate the probability that Watson's mean YPA was greater than Brees' mean YPA by measuring the proportion of values greater than 0 in the above distribution.\n",
    "# We count the number of values greater than 0 and divide by the total number\n",
    "# of observations\n",
    "# which returns us the the proportion of values in the distribution that are\n",
    "# greater than 0\n",
    "(ypa_diff > 0).sum() / ypa_diff.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing multiple players\n",
    "\n",
    "# Creat a list of QBs we are interested in comparing\n",
    "players = ['Tom Brady', 'Jimmy Garoppolo', 'Drew Brees', 'Deshaun Watson', \n",
    "           'Matt Ryan', 'Derek Carr', 'Eli Manning', 'Russell Wilson', \n",
    "           'Dak Prescott', 'Aaron Rodgers']\n",
    "# Grab their passes from our data\n",
    "select_players = qb_pass_df.loc[qb_pass_df.Player.isin(players)]\n",
    "# make sure that each player only has one unique id\n",
    "select_players.groupby(['Player', 'GSIS_ID']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a helper function that we can use to create our bayesian booststrap\n",
    "# samples for each player, calculate the HDI a\n",
    "def create_bb_ypa_df(group, n_replications=10000, alpha=0.05,\n",
    "                     col='Yards_Gained'):\n",
    "    # construct the posterior\n",
    "    posterior = bb.mean(group[col], n_replications=n_replications)\n",
    "    # construct HDI\n",
    "    lower_ci, upper_ci = bb.highest_density_interval(posterior, alpha=0.05)\n",
    "    # get the players observed YPA\n",
    "    pass_yards = group[col].sum()\n",
    "    pass_attempts = len(group[col])\n",
    "    mean_ypa = pass_yards / pass_attempts\n",
    "    # the data we want\n",
    "    data = pd.Series({'posterior': posterior,\n",
    "                      'lower_ci': lower_ci,\n",
    "                      'upper_ci': upper_ci,\n",
    "                      \n",
    "                      # these will make constructing the plot to to compare\n",
    "                      # the players 95% HDI easier\n",
    "                      'lower_ci_diff': lower_ci - mean_ypa,\n",
    "                      'upper_ci_diff': upper_ci - mean_ypa,\n",
    "                      'est_mean_ypa': np.mean(posterior),\n",
    "                      'pass_attempts': pass_attempts,\n",
    "                      'pass_yards': pass_yards,\n",
    "                      'obs_mean_ypa': mean_ypa})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct posteriors for each player by grouping by each player and applying\n",
    "# the above helper function\n",
    "# we should get a dataframe with each player's info\n",
    "with NumpyRNGContext(42):\n",
    "    ypa_df = (select_players.groupby(['GSIS_ID', 'Player'])\n",
    "                            .apply(create_bb_ypa_df)\n",
    "                            .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the dataframe\n",
    "ypa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# league average ypa as a comparison point\n",
    "ypa = qb_pass_df.Yards_Gained.sum() / qb_pass_df.Yards_Gained.size\n",
    "# in order to plot the HDI as error bars around a player's posterior mean\n",
    "# we have to do this transformation of the data and pass it to \n",
    "# matplotlib's errorbar function parameter xerr\n",
    "ypa_ci = np.array(list(zip(-ypa_df.lower_ci_diff, \n",
    "                            ypa_df.upper_ci_diff))).T\n",
    "\n",
    "# now create the plot\n",
    "plt.figure(figsize=(9,12))\n",
    "plt.errorbar('est_mean_ypa', 'Player', xerr=ypa_ci, data=ypa_df, fmt='ko', \n",
    "             capthick=2, capsize=10, label=None)\n",
    "plt.axvline(ypa, color='r', linestyle='--', label='League Avg. YPA')\n",
    "plt.xlabel('Posterior Mean Yards Per Attampet (YPA) with 95% HDI')\n",
    "title = 'Measuring the Uncertainty of a \\nQB\\'s Passing Performance\\n(2017)'\n",
    "plt.title(title)\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode list of psoteriors to make it easier to plot joy plot\n",
    "# https://stackoverflow.com/questions/42012152/unstack-a-pandas-column-containing-lists-into-multiple-rows\n",
    "ypa_posterior_df = pd.DataFrame({'Player': np.repeat(ypa_df.Player, \n",
    "                                                     ypa_df.posterior.str.len()),\n",
    "                                 'posterior': np.concatenate(ypa_df.posterior.values)})\n",
    "\n",
    "# this code is based on the code from the seaborn documentation\n",
    "# https://seaborn.pydata.org/examples/kde_joyplot.html\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(.65, .2, label, fontweight=\"bold\", color=color, \n",
    "            transform=ax.transAxes)\n",
    "\n",
    "# use team colors for each player\n",
    "tm_colors = ['#002244', '#9F8958', '#0B2265', '#FFB612', '#A71930', \n",
    "             '#69BE28', '#000000', '#B3995D', '#00338D', '#03202F']\n",
    "\n",
    "# initialize the FacetGrid object\n",
    "g = sns.FacetGrid(ypa_posterior_df, row=\"Player\", hue=\"Player\", aspect=4.5, \n",
    "                  size=1.3, palette=tm_colors)\n",
    "# plot each player's posterior distribution\n",
    "g.map(sns.kdeplot, \"posterior\", clip_on=False, shade=True, lw=2)\n",
    "# label each distribution with the player's name\n",
    "g.map(label, \"posterior\")\n",
    "# add the the line representing the league avg YPA\n",
    "g.map(plt.axvline, x=ypa, color='k', linestyle='--')\n",
    "# Adjust the spacing for each player's kde plot so that they are veritcally \n",
    "# adjacent to one another\n",
    "g.fig.subplots_adjust(hspace=0)\n",
    "# # Remove axes details that don't play will with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "# add legend, title and xlable\n",
    "ax1 = g.axes[0, 0]\n",
    "ax1.legend([ax1.lines[1]], ['League Avg. YPA'])\n",
    "ax1.set_title(title)\n",
    "plt.xlabel('Yards Per Attempt (YPA)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "For a deeper dive into the Bayesian bootstrap I suggest reading Rasmus Bååth's series of blog posts on the topic. His conference talk is also worth watching. It’s only 15 minutes and he does a good job explaining both the classical bootstrap and the Bayesian bootstrap.\n",
    "\n",
    "His blog posts:\n",
    "- [The Non-parametric Bootstrap as a Bayesian Model](http://sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/)\n",
    "- [Easy Bayesian Bootstrap in R](http://www.sumsar.net/blog/2015/07/easy-bayesian-bootstrap-in-r/)\n",
    "- [bayesboot: An R package for doing the Bayesian bootstrap](http://www.sumsar.net/blog/2016/02/bayesboot-an-r-package/)\n",
    "\n",
    "The video of his talk:\n",
    "- [bayesboot: An R package for easy Bayesian bootstrapping](https://www.youtube.com/watch?v=VceFc5hsMw8&t=)\n",
    "\n",
    "Here are links to github repositories for the R package `bayesboot` and the Python package `bayesian_bootstrap':\n",
    "- [bayesboot](https://github.com/rasmusab/bayesboot)\n",
    "- [bayesian_bootstrap](https://github.com/lmc2179/bayesian_bootstrap)\n",
    "\n",
    "You can find the notebook and the data used for this post on [github](https://github.com/savvastj/nfl_bayesian_bootstrap)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
